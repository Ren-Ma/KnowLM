[2023-07-19 17:54:41,297] [WARNING] [runner.py:191:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-07-19 17:54:41,371] [INFO] [runner.py:541:main] cmd = /root/anaconda3/envs/unichat/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --model_name_or_path /data/caihua/huggingfaceModels/llama/llama-13B --model_max_length 1024 --data_path /data/renma/unigpt//law_data/preproc刑法 --output_dir /data/renma/unigpt//KnowLM/pretrain/output --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 1 --evaluation_strategy no --save_strategy steps --save_steps 100 --save_total_limit 1 --learning_rate 1.5e-5 --warmup_steps 300 --logging_steps 1 --report_to tensorboard --gradient_checkpointing True --deepspeed /data/renma/unigpt//KnowLM/pretrain/configs/config.json --fp16 True --log_on_each_node False --lr_scheduler_type cosine --adam_beta1 0.9 --adam_beta2 0.95 --weight_decay 0.1
[2023-07-19 17:54:43,518] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2023-07-19 17:54:43,518] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=8, node_rank=0
[2023-07-19 17:54:43,518] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2023-07-19 17:54:43,518] [INFO] [launch.py:247:main] dist_world_size=8
[2023-07-19 17:54:43,518] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2023-07-19 17:54:47,839] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-07-19 17:55:14,532] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 13.02B parameters
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.61s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.64s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.68s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.68s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.68s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.70s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.75s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.77s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:09,  9.04s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:09,  9.04s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:09,  9.04s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:09,  9.03s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:09,  9.04s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:09,  9.04s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.07s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.11s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.08s/it]
数据的分布为：[4326]
False
Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.08s/it]
数据的分布为：[4326]
False
Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.09s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.80s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.09s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.09s/it]
数据的分布为：[4326]
False
数据的分布为：[4326]
False
Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.09s/it]
数据的分布为：[4326]
False
数据的分布为：[4326]
False
Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.10s/it]
数据的分布为：[4326]
False
Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.16s/it]
数据的分布为：[4326]
False
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /data/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.4 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /data/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /data/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /data/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[2023-07-19 18:26:54,596] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 69736
[2023-07-19 18:26:54,597] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 69737
[2023-07-19 18:26:55,999] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 69738
[2023-07-19 18:26:57,399] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 69739
[2023-07-19 18:26:58,721] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 69740
[2023-07-19 18:27:00,203] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 69741
[2023-07-19 18:27:01,566] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 69742
[2023-07-19 18:27:03,207] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 69743
[2023-07-19 18:27:05,090] [ERROR] [launch.py:434:sigkill_handler] ['/root/anaconda3/envs/unichat/bin/python3.10', '-u', 'train.py', '--local_rank=7', '--model_name_or_path', '/data/caihua/huggingfaceModels/llama/llama-13B', '--model_max_length', '1024', '--data_path', '/data/renma/unigpt//law_data/preproc刑法', '--output_dir', '/data/renma/unigpt//KnowLM/pretrain/output', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '1', '--learning_rate', '1.5e-5', '--warmup_steps', '300', '--logging_steps', '1', '--report_to', 'tensorboard', '--gradient_checkpointing', 'True', '--deepspeed', '/data/renma/unigpt//KnowLM/pretrain/configs/config.json', '--fp16', 'True', '--log_on_each_node', 'False', '--lr_scheduler_type', 'cosine', '--adam_beta1', '0.9', '--adam_beta2', '0.95', '--weight_decay', '0.1'] exits with return code = -9
